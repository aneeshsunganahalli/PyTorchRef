# -*- coding: utf-8 -*-
"""PyTorchWorkflow.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ATIms7ORkTxOdNm9N_Fi2BzsyzNHpSRT
"""

# PyTorch Workflow
import torch
from torch import nn
import matplotlib.pyplot as plt

weight = 0.7
bias = 0.3

start = 0
end = 1
step = 0.02
X = torch.arange(start,end,step).unsqueeze(dim=1)
y = weight * X + bias
X[:10], y[:10]

trainSplit = int(0.8 * len(X))
trainSplit

XTrain, YTrain = X[:trainSplit], y[:trainSplit]
XTrain, YTrain

# Linear Regression Model Class
class LinearRegressionModel(nn.Module): # almost everything in PyTorch extends from nn.module (base class being extended)
  def __init__(self):
    super().__init__()
    self.weights = nn.Parameter(torch.randn(1,
                                            requires_grad=True,
                                            dtype=torch.float))
    self.bias = nn.Parameter(torch.randn(1,
                                         requires_grad=True,
                                         dtype= torch.float))

    # Define the computation in the model (in this case linear regression)
  def forward(self, x: torch.Tensor) -> torch.Tensor:
    return self.weights * x + self.bias

torch.manual_seed(42)

model0 = LinearRegressionModel()
list(model0.parameters())

model0.state_dict()

# Predicitions
with torch.inference_mode():
  y_preds = model0(XTrain)

y_preds

# Loss Function
loss_fn = nn.L1Loss()

# Optimizer
optimizer = torch.optim.SGD(params= model0.parameters(),
                            lr= 0.01) # learning rate (very imp hyperparameter)

# Epoch is a loop
epochs = 1
# Loop through data
for epoch in range(epochs):
  model0.train()

  # Foward pass
  y_pred = model0(XTrain)

  # Calculate Loss
  loss = nn.L1Loss(y_pred, YTrain)

  # Optimizer zero grad
  optimizer.zero_grad()

  # Perform Back Propogation
  loss.backward()

  # Step the optimizer (perform gradient descent)
  optimizer.step()

  model0.eval()
